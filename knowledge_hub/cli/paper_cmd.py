"""
khub paper - ë…¼ë¬¸ ê°œë³„ ê´€ë¦¬ ëª…ë ¹ì–´

ê°œë³„ ì‘ì—…:
  khub paper add <URL>           URLë¡œ ë…¼ë¬¸ ì¶”ê°€ (arXiv, OpenReview, HuggingFace ë“±)
  khub paper download <ID>       ë‹¨ì¼ ë‹¤ìš´ë¡œë“œ
  khub paper translate <ID>      ë‹¨ì¼ ë²ˆì—­
  khub paper summarize <ID>      ë‹¨ì¼ ìš”ì•½
  khub paper embed <ID>          ë‹¨ì¼ ì„ë² ë”©
  khub paper info <ID>           ìƒì„¸ ì •ë³´

ë°°ì¹˜ ì‘ì—…:
  khub paper translate-all       ë¯¸ë²ˆì—­ ì „ì²´ ë²ˆì—­
  khub paper summarize-all       ë¯¸ìš”ì•½ ì „ì²´ ìš”ì•½
  khub paper embed-all           ë¯¸ì¸ë±ì‹± ì „ì²´ ì„ë² ë”©
  khub paper list                ëª©ë¡ ì¡°íšŒ
"""

from __future__ import annotations

import logging
import os
import re
import time

import click
import requests
from pathlib import Path
from rich.console import Console
from rich.table import Table

console = Console()
log = logging.getLogger("khub.paper")

_ARXIV_ID_RE = re.compile(r"^\d{4}\.\d{4,5}(v\d+)?$")

API_MAX_RETRIES = 3
API_RETRY_BASE_SEC = 2.0


def _validate_arxiv_id(arxiv_id: str) -> str:
    """arXiv ID í˜•ì‹ ê²€ì¦. ìœ íš¨í•˜ë©´ ë°˜í™˜, ì•„ë‹ˆë©´ ClickException."""
    arxiv_id = arxiv_id.strip()
    if not _ARXIV_ID_RE.match(arxiv_id):
        raise click.BadParameter(
            f"ìœ íš¨í•˜ì§€ ì•Šì€ arXiv ID: '{arxiv_id}' (ì˜ˆ: 2501.06322)",
            param_hint="arxiv_id",
        )
    return arxiv_id


def _api_call_with_retry(fn, *args, **kwargs):
    """API í˜¸ì¶œì„ ì¬ì‹œë„í•˜ëŠ” ë²”ìš© ë˜í¼. fnì€ requests í˜¸ì¶œì„ ìˆ˜í–‰í•´ì•¼ í•¨."""
    last_err: Exception | None = None
    for attempt in range(1, API_MAX_RETRIES + 1):
        try:
            return fn(*args, **kwargs)
        except requests.HTTPError as e:
            last_err = e
            status = getattr(e.response, "status_code", 0)
            if status == 429 or status >= 500:
                wait = API_RETRY_BASE_SEC * (2 ** (attempt - 1))
                log.warning("API %d ì—ëŸ¬, %d/%d ì¬ì‹œë„ (%.1fs ëŒ€ê¸°)",
                            status, attempt, API_MAX_RETRIES, wait)
                time.sleep(wait)
                continue
            raise
        except (requests.ConnectionError, requests.Timeout) as e:
            last_err = e
            wait = API_RETRY_BASE_SEC * (2 ** (attempt - 1))
            log.warning("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, %d/%d ì¬ì‹œë„ (%.1fs ëŒ€ê¸°)",
                        attempt, API_MAX_RETRIES, wait)
            time.sleep(wait)
    raise last_err  # type: ignore[misc]


MAX_SUMMARIZE_CHARS = 30000


def _resolve_vault_papers_dir(vault_path: str) -> Path | None:
    """Obsidian vault ë‚´ ë…¼ë¬¸ í´ë”ë¥¼ ë™ì ìœ¼ë¡œ íƒìƒ‰"""
    candidates = [
        Path(vault_path) / "Papers",
        Path(vault_path) / "Projects" / "AI" / "AI_Papers",
        Path(vault_path) / "papers",
    ]
    for c in candidates:
        if c.exists():
            return c
    return Path(vault_path) / "Papers"


def _resolve_vault_concepts_dir(vault_path: str) -> Path:
    """Obsidian vault ë‚´ ê°œë… í´ë”ë¥¼ ë™ì ìœ¼ë¡œ íƒìƒ‰"""
    papers_dir = _resolve_vault_papers_dir(vault_path)
    if papers_dir:
        concepts = papers_dir / "Concepts"
        if concepts.exists():
            return concepts
    candidates = [
        Path(vault_path) / "Papers" / "Concepts",
        Path(vault_path) / "Projects" / "AI" / "AI_Papers" / "Concepts",
        Path(vault_path) / "Concepts",
    ]
    for c in candidates:
        if c.exists():
            return c
    return (papers_dir or Path(vault_path) / "Papers") / "Concepts"


def _collect_paper_text(paper: dict, config) -> str:
    """ë…¼ë¬¸ ì „ë¬¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘: ë²ˆì—­ë³¸ â†’ ì›ë¬¸ í…ìŠ¤íŠ¸ â†’ abstract ìˆœìœ¼ë¡œ fallback"""
    translated = paper.get("translated_path")
    if translated and Path(translated).exists():
        text = Path(translated).read_text(encoding="utf-8")
        if len(text) > 200:
            return text[:MAX_SUMMARIZE_CHARS]

    text_path = paper.get("text_path")
    if text_path and Path(text_path).exists():
        text = Path(text_path).read_text(encoding="utf-8")
        if len(text) > 200:
            return text[:MAX_SUMMARIZE_CHARS]

    papers_dir = Path(config.papers_dir)
    for pattern in [f"*{paper['arxiv_id']}*.txt", f"*{paper['title'][:30]}*.txt"]:
        for p in papers_dir.glob(pattern):
            text = p.read_text(encoding="utf-8")
            if len(text) > 200:
                return text[:MAX_SUMMARIZE_CHARS]

    title = paper.get("title", "")
    authors = paper.get("authors", "")
    field = paper.get("field", "")
    notes = paper.get("notes", "")
    return f"ì œëª©: {title}\nì €ì: {authors}\në¶„ì•¼: {field}\n{notes}"


def _update_obsidian_summary(paper: dict, summary: str, config):
    """Obsidian ë…¸íŠ¸ê°€ ìˆìœ¼ë©´ ìš”ì•½ ì„¹ì…˜ì„ ì—…ë°ì´íŠ¸"""
    if not config.vault_path:
        return
    vault = Path(config.vault_path)
    safe_title = re.sub(r'[\\/:*?"<>|]', '', paper["title"]).strip()
    safe_title = re.sub(r'\s+', ' ', safe_title)[:100].strip()

    papers_dir = _resolve_vault_papers_dir(str(vault))
    if papers_dir:
        note_path = papers_dir / f"{safe_title}.md"
        if not note_path.exists():
            return
        content = note_path.read_text(encoding="utf-8")
        placeholder = "ìš”ì•½ë³¸/ë²ˆì—­ë³¸ì´ ì•„ì§ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        old_summary_section = None

        if placeholder in content:
            content = content.replace(placeholder, summary)
            note_path.write_text(content, encoding="utf-8")
            console.print(f"[dim]Obsidian ë…¸íŠ¸ ì—…ë°ì´íŠ¸: {note_path.name}[/dim]")
            return

        if "## ìš”ì•½" in content:
            lines = content.split("\n")
            start = None
            end = None
            for i, line in enumerate(lines):
                if line.strip() == "## ìš”ì•½":
                    start = i
                elif start is not None and line.startswith("## ") and i > start:
                    end = i
                    break
            if start is not None:
                if end is None:
                    end = len(lines)
                new_lines = lines[:start] + ["## ìš”ì•½", "", summary, ""] + lines[end:]
                note_path.write_text("\n".join(new_lines), encoding="utf-8")
                console.print(f"[dim]Obsidian ë…¸íŠ¸ ì—…ë°ì´íŠ¸: {note_path.name}[/dim]")
                return


@click.group("paper")
def paper_group():
    """ë…¼ë¬¸ ê´€ë¦¬ (add/download/translate/summarize/embed/list/info)"""
    pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper list
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("list")
@click.option("--field", "-f", default=None, help="ë¶„ì•¼ í•„í„°")
@click.option("--limit", "-n", default=50, help="í‘œì‹œí•  ìµœëŒ€ ìˆ˜")
@click.pass_context
def paper_list(ctx, field, limit):
    """ìˆ˜ì§‘ëœ ë…¼ë¬¸ ëª©ë¡"""
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    papers = sqlite_db.list_papers(field=field, limit=limit)

    if not papers:
        console.print("[yellow]ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. khub discoverë¡œ ì‹œì‘í•˜ì„¸ìš”.[/yellow]")
        return

    table = Table(title=f"ë…¼ë¬¸ ëª©ë¡ ({len(papers)}ê°œ)")
    table.add_column("arXiv ID", style="cyan", width=14)
    table.add_column("ì œëª©", max_width=50)
    table.add_column("ì—°ë„", width=5)
    table.add_column("ë¶„ì•¼", style="magenta", max_width=20)
    table.add_column("PDF", width=4)
    table.add_column("ìš”ì•½", width=4)
    table.add_column("ë²ˆì—­", width=4)
    table.add_column("ë²¡í„°", width=4)

    for p in papers:
        notes = p.get("notes") or ""
        has_summary = len(notes) > 30
        table.add_row(
            p["arxiv_id"],
            p["title"][:50],
            str(p.get("year", "")),
            p.get("field", "")[:20],
            "[green]O[/green]" if p.get("pdf_path") else "-",
            "[green]O[/green]" if has_summary else "-",
            "[green]O[/green]" if p.get("translated_path") else "-",
            "[green]O[/green]" if p.get("indexed") else "-",
        )

    console.print(table)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper add <URL>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("add")
@click.argument("url")
@click.option("--download/--no-download", default=True, help="PDF ë‹¤ìš´ë¡œë“œ ì—¬ë¶€")
@click.pass_context
def paper_add(ctx, url, download):
    """URLë¡œ ë…¼ë¬¸ ì¶”ê°€ (arXiv, OpenReview, PapersWithCode, HuggingFace, S2, PDF URL)"""
    config = ctx.obj["khub"].config
    from knowledge_hub.papers.url_resolver import resolve_url
    from knowledge_hub.papers.downloader import PaperDownloader
    from knowledge_hub.core.database import SQLiteDatabase

    with console.status(f"[cyan]URL ë¶„ì„ ì¤‘: {url[:60]}...[/cyan]"):
        paper = resolve_url(url)

    if not paper:
        console.print("[red]ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
        return

    console.print(f"[bold]{paper.title}[/bold]")
    console.print(f"  ì €ì: {paper.authors}")
    console.print(f"  ì—°ë„: {paper.year} | ì¸ìš©: {paper.citation_count} | ì†ŒìŠ¤: {paper.source}")
    if paper.abstract:
        console.print(f"  ì´ˆë¡: {paper.abstract[:120]}...")

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    existing = sqlite_db.get_paper(paper.arxiv_id) if paper.arxiv_id else None
    if existing:
        console.print(f"[yellow]ì´ë¯¸ ë“±ë¡ëœ ë…¼ë¬¸ì…ë‹ˆë‹¤: {paper.arxiv_id}[/yellow]")
        return

    paper_data = {
        "arxiv_id": paper.arxiv_id or re.sub(r'[^\w]', '_', paper.title)[:30],
        "title": paper.title,
        "authors": paper.authors,
        "year": paper.year,
        "field": ", ".join(paper.fields_of_study[:3]),
        "importance": 3,
        "notes": f"citations: {paper.citation_count}",
        "pdf_path": None,
        "text_path": None,
        "translated_path": None,
    }

    if download:
        downloader = PaperDownloader(config.papers_dir)
        with console.status("ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            result = downloader.download_single(paper.arxiv_id, paper.title)
        paper_data["pdf_path"] = result.get("pdf")
        paper_data["text_path"] = result.get("text")
        if result["success"]:
            console.print(f"  [green]PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ[/green]")
        else:
            console.print(f"  [yellow]PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨[/yellow]")

    sqlite_db.upsert_paper(paper_data)
    console.print(f"[green]ë…¼ë¬¸ ë“±ë¡ ì™„ë£Œ: {paper_data['arxiv_id']}[/green]")
    console.print("[dim]khub paper summarize / translate / embed ë¡œ í›„ì† ì‘ì—… ê°€ëŠ¥[/dim]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper download <arxiv_id>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("download")
@click.argument("arxiv_id")
@click.pass_context
def paper_download(ctx, arxiv_id):
    """ë‹¨ì¼ ë…¼ë¬¸ PDF/í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ"""
    arxiv_id = _validate_arxiv_id(arxiv_id)
    config = ctx.obj["khub"].config
    from knowledge_hub.papers.downloader import PaperDownloader
    from knowledge_hub.core.database import SQLiteDatabase

    downloader = PaperDownloader(config.papers_dir)
    sqlite_db = SQLiteDatabase(config.sqlite_path)

    existing = sqlite_db.get_paper(arxiv_id)
    title = existing["title"] if existing else arxiv_id

    try:
        with console.status(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {arxiv_id}..."):
            result = downloader.download_single(arxiv_id, title)
    except Exception as e:
        console.print(f"[red]ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}[/red]")
        return

    if result["success"]:
        paper_data = {
            "arxiv_id": arxiv_id,
            "title": title,
            "authors": existing.get("authors", "") if existing else "",
            "year": existing.get("year", 0) if existing else 0,
            "field": existing.get("field", "") if existing else "",
            "importance": existing.get("importance", 3) if existing else 3,
            "notes": existing.get("notes", "") if existing else "",
            "pdf_path": result.get("pdf"),
            "text_path": result.get("text"),
            "translated_path": existing.get("translated_path") if existing else None,
        }
        sqlite_db.upsert_paper(paper_data)
        console.print(f"[green]ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {result.get('pdf', 'N/A')}[/green]")
    else:
        console.print(f"[red]ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {arxiv_id}[/red]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper translate <arxiv_id>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("translate")
@click.argument("arxiv_id")
@click.option("--provider", "-p", default=None, help="ë²ˆì—­ í”„ë¡œë°”ì´ë” (ê¸°ë³¸: config)")
@click.option("--model", "-m", default=None, help="ë²ˆì—­ ëª¨ë¸ (ê¸°ë³¸: config)")
@click.pass_context
def paper_translate(ctx, arxiv_id, provider, model):
    """ë…¼ë¬¸ ì „ì²´ ë²ˆì—­ (arXiv ID ì§€ì •)"""
    arxiv_id = _validate_arxiv_id(arxiv_id)
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    paper = sqlite_db.get_paper(arxiv_id)

    if not paper:
        console.print(f"[red]ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arxiv_id}[/red]")
        return

    text_path = paper.get("text_path")
    if not text_path:
        console.print("[red]í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. khub paper download ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.[/red]")
        return

    prov = provider or config.translation_provider
    mdl = model or config.translation_model

    console.print(f"ë²ˆì—­ ì¤‘: [bold]{paper['title'][:60]}[/bold]")
    console.print(f"[dim]í”„ë¡œë°”ì´ë”: {prov}/{mdl}[/dim]")

    from knowledge_hub.providers.registry import get_llm

    llm = get_llm(prov, model=mdl, **config.get_provider_config(prov))

    try:
        text = Path(text_path).read_text(encoding="utf-8")
    except Exception as e:
        console.print(f"[red]í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}[/red]")
        return

    output_dir = Path(config.papers_dir) / "translated"
    output_dir.mkdir(parents=True, exist_ok=True)

    safe_title = re.sub(r'[\\/:*?"<>|]', '', paper['title']).strip()
    safe_title = re.sub(r'\s+', ' ', safe_title)[:100].strip()
    output_path = output_dir / f"{safe_title}_translated.md"

    chunk_size = 6000
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    translated_parts = []
    for i, chunk in enumerate(chunks):
        console.print(f"  [{i + 1}/{len(chunks)}] ë²ˆì—­ ì¤‘...")
        result = llm.translate(chunk, source_lang="en", target_lang="ko")
        translated_parts.append(result)

    full_translation = "\n\n".join(translated_parts)
    header = f"# {paper['title']}\n\n> arXiv: {arxiv_id} | ë²ˆì—­: {prov}/{mdl}\n\n---\n\n"
    output_path.write_text(header + full_translation, encoding="utf-8")

    sqlite_db.conn.execute(
        "UPDATE papers SET translated_path = ? WHERE arxiv_id = ?",
        (str(output_path), arxiv_id),
    )
    sqlite_db.conn.commit()
    console.print(f"[green]ë²ˆì—­ ì™„ë£Œ: {output_path.name}[/green]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper summarize <arxiv_id>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("summarize")
@click.argument("arxiv_id")
@click.option("--provider", "-p", default=None, help="ìš”ì•½ í”„ë¡œë°”ì´ë” (ê¸°ë³¸: config)")
@click.option("--model", "-m", default=None, help="ìš”ì•½ ëª¨ë¸ (ê¸°ë³¸: config)")
@click.option("--quick", is_flag=True, help="ê°„ë‹¨ ìš”ì•½ (5ë¬¸ì¥, abstractë§Œ ì‚¬ìš©)")
@click.pass_context
def paper_summarize(ctx, arxiv_id, provider, model, quick):
    """ë…¼ë¬¸ ì‹¬ì¸µ ìš”ì•½ ìƒì„± (êµ¬ì¡°í™”ëœ ë¶„ì„)"""
    arxiv_id = _validate_arxiv_id(arxiv_id)
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    paper = sqlite_db.get_paper(arxiv_id)

    if not paper:
        console.print(f"[red]ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arxiv_id}[/red]")
        return

    prov = provider or config.summarization_provider
    mdl = model or config.summarization_model

    console.print(f"ìš”ì•½ ì¤‘: [bold]{paper['title'][:60]}[/bold]")
    console.print(f"[dim]í”„ë¡œë°”ì´ë”: {prov}/{mdl}[/dim]")

    from knowledge_hub.providers.registry import get_llm
    llm = get_llm(prov, model=mdl, **config.get_provider_config(prov))

    text = _collect_paper_text(paper, config)
    source_label = "ì „ë¬¸" if len(text) > 2000 else "abstract"
    console.print(f"[dim]ì…ë ¥ ì†ŒìŠ¤: {source_label} ({len(text):,}ì)[/dim]")

    with console.status("ì‹¬ì¸µ ìš”ì•½ ìƒì„± ì¤‘..."):
        if quick:
            summary = llm.summarize(text, language="ko", max_sentences=5)
        else:
            summary = llm.summarize_paper(text, title=paper["title"], language="ko")

    console.print(f"\n[bold]ìš”ì•½: {paper['title']}[/bold]\n")
    from rich.markdown import Markdown
    console.print(Markdown(summary))

    sqlite_db.conn.execute(
        "UPDATE papers SET notes = ? WHERE arxiv_id = ?",
        (summary, arxiv_id),
    )
    sqlite_db.conn.commit()

    _update_obsidian_summary(paper, summary, config)
    console.print(f"\n[green]ìš”ì•½ ì €ì¥ ì™„ë£Œ[/green]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper embed <arxiv_id>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("embed")
@click.argument("arxiv_id")
@click.pass_context
def paper_embed(ctx, arxiv_id):
    """ë‹¨ì¼ ë…¼ë¬¸ ë²¡í„° ì„ë² ë”©"""
    arxiv_id = _validate_arxiv_id(arxiv_id)
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase, VectorDatabase

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    paper = sqlite_db.get_paper(arxiv_id)

    if not paper:
        console.print(f"[red]ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arxiv_id}[/red]")
        return

    console.print(f"ì„ë² ë”© ì¤‘: [bold]{paper['title'][:60]}[/bold]")

    text = f"Title: {paper['title']}"
    if paper.get("notes"):
        text += f"\n\n{paper['notes']}"

    from knowledge_hub.providers.registry import get_embedder as _get_embedder
    try:
        embed_cfg = config.get_provider_config(config.embedding_provider)
        embedder = _get_embedder(config.embedding_provider, model=config.embedding_model, **embed_cfg)
        emb = embedder.embed_text(text)
    except Exception as e:
        console.print(f"[red]ì„ë² ë”© ì‹¤íŒ¨: {e}[/red]")
        return

    vector_db = VectorDatabase(config.vector_db_path, config.collection_name)
    vector_db.add_documents(
        documents=[text],
        embeddings=[emb],
        metadatas=[{
            "title": paper["title"],
            "arxiv_id": arxiv_id,
            "source_type": "paper",
            "field": paper.get("field", ""),
            "chunk_index": 0,
        }],
        ids=[f"paper_{arxiv_id}_0"],
    )

    sqlite_db.conn.execute("UPDATE papers SET indexed = 1 WHERE arxiv_id = ?", (arxiv_id,))
    sqlite_db.conn.commit()
    console.print(f"[green]ì„ë² ë”© ì™„ë£Œ (ë²¡í„°DB: {vector_db.count()}ê°œ ë¬¸ì„œ)[/green]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper translate-all
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("translate-all")
@click.option("--limit", "-n", default=0, help="ìµœëŒ€ ë²ˆì—­ ìˆ˜ (0=ì „ì²´)")
@click.option("--field", "-f", default=None, help="ë¶„ì•¼ í•„í„°")
@click.option("--provider", "-p", default=None, help="ë²ˆì—­ í”„ë¡œë°”ì´ë”")
@click.option("--model", "-m", default=None, help="ë²ˆì—­ ëª¨ë¸")
@click.pass_context
def paper_translate_all(ctx, limit, field, provider, model):
    """ë¯¸ë²ˆì—­ ë…¼ë¬¸ ì „ì²´ ë²ˆì—­"""
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    papers = sqlite_db.list_papers(field=field, limit=999)
    untranslated = [p for p in papers if not p.get("translated_path") and p.get("text_path")]

    if limit > 0:
        untranslated = untranslated[:limit]

    if not untranslated:
        console.print("[green]ëª¨ë“  ë…¼ë¬¸ì´ ì´ë¯¸ ë²ˆì—­ë˜ì—ˆê±°ë‚˜ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.[/green]")
        return

    prov = provider or config.translation_provider
    mdl = model or config.translation_model

    console.print(f"[bold]ë¯¸ë²ˆì—­ ë…¼ë¬¸ {len(untranslated)}í¸ ë²ˆì—­ ì‹œì‘[/bold]")
    console.print(f"[dim]í”„ë¡œë°”ì´ë”: {prov}/{mdl}[/dim]\n")

    from knowledge_hub.providers.registry import get_llm
    llm = get_llm(prov, model=mdl, **config.get_provider_config(prov))

    output_dir = Path(config.papers_dir) / "translated"
    output_dir.mkdir(parents=True, exist_ok=True)
    success = 0
    failed: list[dict] = []

    for idx, paper in enumerate(untranslated, 1):
        aid = paper["arxiv_id"]
        title = paper["title"]
        console.print(f"[{idx}/{len(untranslated)}] {title[:55]}...", end=" ")

        try:
            text = Path(paper["text_path"]).read_text(encoding="utf-8")
        except Exception as e:
            console.print(f"[red]ì½ê¸° ì‹¤íŒ¨: {e}[/red]")
            failed.append({"arxiv_id": aid, "error": f"íŒŒì¼ ì½ê¸°: {e}"})
            continue

        chunk_size = 6000
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        translated_parts = []
        chunk_failed = False
        for ci, chunk in enumerate(chunks):
            try:
                translated_parts.append(llm.translate(chunk, source_lang="en", target_lang="ko"))
            except Exception as e:
                log.error("ë²ˆì—­ ì‹¤íŒ¨ %s ì²­í¬ %d: %s", aid, ci, e)
                console.print(f"[red]ì²­í¬ {ci+1} ì‹¤íŒ¨[/red]")
                failed.append({"arxiv_id": aid, "error": f"ì²­í¬ {ci+1}: {e}"})
                chunk_failed = True
                break

        if chunk_failed:
            continue

        safe_title = re.sub(r'[\\/:*?"<>|]', '', title).strip()
        safe_title = re.sub(r'\s+', ' ', safe_title)[:100].strip()
        out_path = output_dir / f"{safe_title}_translated.md"

        header = f"# {title}\n\n> arXiv: {aid} | ë²ˆì—­: {prov}/{mdl}\n\n---\n\n"
        out_path.write_text(header + "\n\n".join(translated_parts), encoding="utf-8")

        sqlite_db.conn.execute(
            "UPDATE papers SET translated_path = ? WHERE arxiv_id = ?",
            (str(out_path), aid),
        )
        sqlite_db.conn.commit()
        success += 1
        console.print(f"[green]OK ({len(chunks)}ì²­í¬)[/green]")

    console.print(f"\n[bold green]{success}/{len(untranslated)}í¸ ë²ˆì—­ ì™„ë£Œ[/bold green]")
    if failed:
        console.print(f"[bold red]âš  ì‹¤íŒ¨: {len(failed)}í¸[/bold red]")
        for f in failed:
            console.print(f"  {f['arxiv_id']}: {f['error'][:80]}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper summarize-all
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("summarize-all")
@click.option("--limit", "-n", default=0, help="ìµœëŒ€ ìš”ì•½ ìˆ˜ (0=ì „ì²´)")
@click.option("--field", "-f", default=None, help="ë¶„ì•¼ í•„í„°")
@click.option("--quick", is_flag=True, help="ê°„ë‹¨ ìš”ì•½ (êµ¬ì¡°í™” ë¶„ì„ ëŒ€ì‹  3-5ë¬¸ì¥)")
@click.option("--resummary", is_flag=True, help="ì´ë¯¸ ìš”ì•½ëœ ë…¼ë¬¸ë„ ì¬ìš”ì•½")
@click.pass_context
def paper_summarize_all(ctx, limit, field, quick, resummary):
    """ì „ì²´ ë…¼ë¬¸ ì‹¬ì¸µ ìš”ì•½ (êµ¬ì¡°í™”ëœ ë¶„ì„)"""
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase
    from knowledge_hub.providers.registry import get_llm

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    papers = sqlite_db.list_papers(field=field, limit=999)

    if resummary:
        targets = papers
    else:
        targets = [p for p in papers if not p.get("notes") or len(p.get("notes", "")) < 100]

    if limit > 0:
        targets = targets[:limit]

    if not targets:
        console.print("[green]ëª¨ë“  ë…¼ë¬¸ì´ ì´ë¯¸ ìš”ì•½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.[/green]")
        return

    prov = config.summarization_provider
    mdl = config.summarization_model
    llm = get_llm(prov, model=mdl, **config.get_provider_config(prov))

    console.print(f"[bold]{len(targets)}í¸ {'ê°„ë‹¨' if quick else 'ì‹¬ì¸µ'} ìš”ì•½ ì‹œì‘[/bold]")
    console.print(f"[dim]í”„ë¡œë°”ì´ë”: {prov}/{mdl}[/dim]\n")

    # abstractê°€ ì—†ëŠ” ë…¼ë¬¸ì€ Semantic Scholarì—ì„œ ë³´ì¶©
    missing_abstract = [p for p in targets if not _collect_paper_text(p, config) or len(_collect_paper_text(p, config)) < 100]
    if missing_abstract:
        aids = [p["arxiv_id"] for p in missing_abstract]
        abstract_map = {}
        for i in range(0, len(aids), 50):
            chunk = aids[i:i+50]
            try:
                resp = requests.post(
                    "https://api.semanticscholar.org/graph/v1/paper/batch",
                    params={"fields": "title,abstract,externalIds"},
                    json={"ids": [f"ArXiv:{a}" for a in chunk]},
                    timeout=60,
                )
                if resp.status_code == 200:
                    for paper_data in resp.json():
                        if paper_data and paper_data.get("abstract"):
                            ext = paper_data.get("externalIds", {})
                            aid = ext.get("ArXiv", "")
                            if aid:
                                abstract_map[aid] = paper_data["abstract"]
            except Exception:
                pass
        console.print(f"[dim]Semantic Scholarì—ì„œ {len(abstract_map)}í¸ abstract ë³´ì¶©[/dim]\n")

    success = 0
    failed: list[dict] = []
    for idx, p in enumerate(targets, 1):
        aid = p["arxiv_id"]
        title = p["title"]

        text = _collect_paper_text(p, config)
        if len(text) < 100 and 'abstract_map' in dir():
            extra = abstract_map.get(aid, "")
            if extra:
                text = f"ì œëª©: {title}\nì´ˆë¡: {extra}"

        if len(text) < 50:
            console.print(f"  [{idx}/{len(targets)}] {aid} - í…ìŠ¤íŠ¸ ë¶€ì¡±, ìŠ¤í‚µ")
            continue

        source = "ì „ë¬¸" if len(text) > 2000 else "abstract"
        console.print(f"  [{idx}/{len(targets)}] {title[:50]}... ({source})", end=" ")

        try:
            if quick:
                summary = llm.summarize(text, language="ko", max_sentences=5)
            else:
                summary = llm.summarize_paper(text, title=title, language="ko")

            sqlite_db.conn.execute(
                "UPDATE papers SET notes = ? WHERE arxiv_id = ?",
                (summary, aid),
            )
            sqlite_db.conn.commit()

            _update_obsidian_summary(p, summary, config)
            success += 1
            console.print("[green]OK[/green]")
        except Exception as e:
            log.error("ìš”ì•½ ì‹¤íŒ¨ %s: %s", aid, e)
            failed.append({"arxiv_id": aid, "error": str(e)})
            console.print(f"[red]FAIL ({e})[/red]")

    console.print(f"\n[bold green]{success}/{len(targets)}í¸ ìš”ì•½ ì™„ë£Œ[/bold green]")
    if failed:
        console.print(f"[bold red]ì‹¤íŒ¨: {len(failed)}í¸[/bold red]")
        for f in failed:
            console.print(f"  {f['arxiv_id']}: {f['error'][:80]}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper embed-all
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("embed-all")
@click.option("--all", "index_all", is_flag=True, help="ì´ë¯¸ ì¸ë±ì‹±ëœ ë…¼ë¬¸ë„ ì¬ì¸ë±ì‹±")
@click.pass_context
def paper_embed_all(ctx, index_all):
    """ë¯¸ì¸ë±ì‹± ë…¼ë¬¸ ì „ì²´ ë²¡í„° ì„ë² ë”©"""
    from knowledge_hub.core.database import SQLiteDatabase, VectorDatabase

    config = ctx.obj["khub"].config
    sqlite_db = SQLiteDatabase(config.sqlite_path)
    papers = sqlite_db.list_papers(limit=999)
    unindexed = papers if index_all else [p for p in papers if not p.get("indexed")]

    if not unindexed:
        console.print("[green]ëª¨ë“  ë…¼ë¬¸ì´ ì´ë¯¸ ì¸ë±ì‹±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.[/green]")
        return

    console.print(f"[bold]ì¸ë±ì‹± ì‹œì‘: {len(unindexed)}í¸[/bold]")
    console.print(f"[dim]ì„ë² ë”©: {config.embedding_provider}/{config.embedding_model}[/dim]")

    from knowledge_hub.providers.registry import get_embedder as _get_embedder
    embed_cfg = config.get_provider_config(config.embedding_provider)
    embedder = _get_embedder(config.embedding_provider, model=config.embedding_model, **embed_cfg)

    vector_db = VectorDatabase(config.vector_db_path, config.collection_name)
    batch_size = 20
    success = 0
    t_start = time.time()

    for i in range(0, len(unindexed), batch_size):
        batch = unindexed[i:i + batch_size]
        texts = []
        for p in batch:
            t = f"Title: {p['title'] or p['arxiv_id']}"
            if p.get("notes"):
                t += f"\n\n{p['notes']}"
            texts.append(t)

        try:
            raw_embs = embedder.embed_batch(texts, show_progress=False)
            embs = [e for e in raw_embs if e is not None]
            if len(embs) != len(texts):
                raise RuntimeError(f"{len(texts) - len(embs)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨")

            docs, embeddings, metas, ids = [], [], [], []
            for p, text, emb in zip(batch, texts, embs):
                docs.append(text)
                embeddings.append(emb)
                metas.append({
                    "title": p["title"] or "",
                    "arxiv_id": p["arxiv_id"],
                    "source_type": "paper",
                    "field": p.get("field", ""),
                    "chunk_index": 0,
                })
                ids.append(f"paper_{p['arxiv_id']}_0")

            vector_db.add_documents(documents=docs, embeddings=embeddings, metadatas=metas, ids=ids)

            for p in batch:
                sqlite_db.conn.execute("UPDATE papers SET indexed = 1 WHERE arxiv_id = ?", (p["arxiv_id"],))
            sqlite_db.conn.commit()

            success += len(batch)
            console.print(f"  [{success}/{len(unindexed)}] ë°°ì¹˜: [green]{len(batch)}í¸ OK[/green]")
        except Exception as e:
            console.print(f"  ë°°ì¹˜ ì‹¤íŒ¨: [red]{e}[/red]")

    elapsed = time.time() - t_start
    console.print(f"\n[bold green]{success}/{len(unindexed)}í¸ ì¸ë±ì‹± ì™„ë£Œ ({elapsed:.1f}ì´ˆ)[/bold green]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper info <arxiv_id>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper sync-keywords
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("sync-keywords")
@click.option("--force", is_flag=True, help="ì´ë¯¸ í‚¤ì›Œë“œê°€ ìˆëŠ” ë…¼ë¬¸ë„ ì¬ì¶”ì¶œ")
@click.option("--limit", "-n", default=0, help="ìµœëŒ€ ì²˜ë¦¬ ìˆ˜ (0=ì „ì²´)")
@click.pass_context
def paper_sync_keywords(ctx, force, limit):
    """ëª¨ë“  ë…¼ë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ+ê·¼ê±° ì¶”ì¶œ â†’ kg_relations + Obsidian ë…¸íŠ¸ ê°±ì‹ """
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase
    import json

    vault_path = config.vault_path
    if not vault_path:
        console.print("[red]Obsidian vault ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. khub config set obsidian.vault_path <ê²½ë¡œ>[/red]")
        return

    papers_dir = _resolve_vault_papers_dir(vault_path)
    if not papers_dir or not papers_dir.exists():
        console.print(f"[red]Obsidian ë…¼ë¬¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
        console.print("[dim]khub config set obsidian.vault_path ë¡œ vault ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.[/dim]")
        return

    from knowledge_hub.providers.registry import get_llm
    prov = config.summarization_provider
    mdl = config.summarization_model
    prov_cfg = config.get_provider_config(prov)

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    md_files = sorted(papers_dir.glob("*.md"))
    md_files = [f for f in md_files if f.name != "00_Concept_Index.md"]

    console.print(f"[bold]Obsidian ë…¼ë¬¸ ë…¸íŠ¸ {len(md_files)}ê°œ ìŠ¤ìº” ì¤‘...[/bold]\n")

    all_concepts: dict[str, list[str]] = {}
    updated = 0
    skipped = 0
    relations_added = 0

    for idx, md_path in enumerate(md_files):
        content = md_path.read_text(encoding="utf-8")

        arxiv_match = re.search(r'arxiv_id:\s*"?([0-9]+\.[0-9]+)"?', content)
        arxiv_id = arxiv_match.group(1) if arxiv_match else None

        has_good_concepts = "ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë…" in content and "[[" in content.split("ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë…")[-1].split("#")[0] if "ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë…" in content else False

        if has_good_concepts and not force:
            concepts_section = content.split("ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë…")[-1]
            next_heading = concepts_section.find("\n# ")
            if next_heading > 0:
                concepts_section = concepts_section[:next_heading]
            concepts = re.findall(r'\[\[([^\]]+)\]\]', concepts_section)
            concepts = [c for c in concepts if c != "00_Concept_Index"]
            for c in concepts:
                all_concepts.setdefault(c, []).append(md_path.stem)
            skipped += 1
            continue

        if limit > 0 and updated >= limit:
            break

        title = md_path.stem
        summary_text = _extract_summary_text(content, title, sqlite_db)

        if not summary_text or len(summary_text) < 20:
            console.print(f"  [{idx+1}/{len(md_files)}] {title[:50]}... [dim]í…ìŠ¤íŠ¸ ë¶€ì¡±, ìŠ¤í‚µ[/dim]")
            skipped += 1
            continue

        console.print(f"  [{idx+1}/{len(md_files)}] {title[:50]}...", end=" ")

        try:
            if not hasattr(paper_sync_keywords, '_llm'):
                paper_sync_keywords._llm = get_llm(prov, model=mdl, **prov_cfg)
            evidence_results = _extract_keywords_with_evidence(paper_sync_keywords._llm, title, summary_text, sqlite_db)
        except Exception as e:
            console.print(f"[red]ì‹¤íŒ¨: {e}[/red]")
            continue

        if not evidence_results:
            console.print("[yellow]í‚¤ì›Œë“œ ì—†ìŒ[/yellow]")
            continue

        concepts = [e["concept"] for e in evidence_results]
        for c in concepts:
            all_concepts.setdefault(c, []).append(title)

        # kg_relationsì— paper_uses_concept ê´€ê³„ + ê·¼ê±° ì €ì¥
        if arxiv_id:
            for ev in evidence_results:
                cname = ev["concept"]
                cid = _concept_id(cname)
                sqlite_db.upsert_concept(cid, cname)
                sqlite_db.add_relation(
                    source_type="paper", source_id=arxiv_id,
                    relation="paper_uses_concept",
                    target_type="concept", target_id=cid,
                    evidence_text=ev.get("evidence", ""),
                    confidence=ev.get("confidence", 0.7),
                )
                relations_added += 1

        new_content = _update_note_concepts(content, concepts)
        md_path.write_text(new_content, encoding="utf-8")
        updated += 1
        console.print(f"[green]{len(concepts)}ê°œ í‚¤ì›Œë“œ[/green]")

    console.print(f"\n[bold]ì—…ë°ì´íŠ¸: {updated}ê°œ | ìŠ¤í‚µ(ê¸°ì¡´): {skipped}ê°œ | ê´€ê³„: {relations_added}ê°œ[/bold]")

    concept_index_path = papers_dir / "00_Concept_Index.md"
    _regenerate_concept_index(concept_index_path, all_concepts)
    console.print(f"[bold green]Concept Index ê°±ì‹  ì™„ë£Œ ({len(all_concepts)}ê°œ ê°œë…)[/bold green]")


def _extract_summary_text(content: str, title: str, sqlite_db) -> str:
    """ë…¸íŠ¸ì—ì„œ ìš”ì•½/ì´ˆë¡ í…ìŠ¤íŠ¸ ì¶”ì¶œ, ì—†ìœ¼ë©´ DBì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
    placeholder = "ìš”ì•½ë³¸/ë²ˆì—­ë³¸ì´ ì•„ì§ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

    for heading in ["## ìš”ì•½", "# ğŸ“Œ í•œì¤„ ìš”ì•½", "## ì´ˆë¡"]:
        if heading in content:
            section = content.split(heading, 1)[1]
            next_h = re.search(r'\n#{1,3} ', section)
            if next_h:
                section = section[:next_h.start()]
            section = section.strip()
            if section and placeholder not in section and len(section) > 20:
                return section[:3000]

    arxiv_match = re.search(r'arxiv_id:\s*"?([0-9]+\.[0-9]+)"?', content)
    if arxiv_match:
        aid = arxiv_match.group(1)
        paper = sqlite_db.get_paper(aid)
        if paper:
            notes = paper.get("notes", "")
            if notes and len(notes) > 30:
                return f"ì œëª©: {paper.get('title', title)}\në¶„ì•¼: {paper.get('field', '')}\n{notes}"[:3000]

    return f"ì œëª©: {title}"


def _extract_keywords_with_evidence(llm, title: str, text: str,
                                     sqlite_db=None) -> list[dict]:
    """LLMìœ¼ë¡œ í‚¤ì›Œë“œ + ê·¼ê±° ë¬¸ì¥ì„ í•¨ê»˜ ì¶”ì¶œ.

    ë°˜í™˜: [{"concept": "Transformer", "evidence": "We propose...", "confidence": 0.9}, ...]
    """
    import json as _json

    prompt = (
        "You extract 5-10 core academic concepts from AI/ML papers. "
        "For each concept, provide a short evidence sentence from the text that "
        "shows why this concept is relevant to this paper, plus a confidence score.\n\n"
        "Return ONLY valid JSON: [{\"concept\": \"Name\", \"evidence\": \"sentence\", \"confidence\": 0.9}, ...]\n\n"
        "Rules:\n"
        "- Use SINGULAR form (e.g. 'Neural Network' not 'Neural Networks')\n"
        "- Use full names, not abbreviations\n"
        "- Use standard academic terms\n"
        "- confidence: 0.5-1.0 based on how central the concept is to this paper\n"
        "- evidence: 1 sentence from the text, or a brief paraphrase if exact quote unavailable\n\n"
        f"Paper: {title}\n\n{text[:2500]}"
    )
    raw = llm.generate(prompt).strip()
    if raw.startswith("```"):
        raw = re.sub(r"^```\w*\n?", "", raw)
        raw = re.sub(r"\n?```$", "", raw)
    items = _json.loads(raw)
    if not isinstance(items, list):
        return []

    results = []
    seen = set()
    for item in items:
        if not isinstance(item, dict) or "concept" not in item:
            continue
        name = str(item["concept"]).strip()
        if not name or len(name) <= 1:
            continue
        if sqlite_db:
            canonical = sqlite_db.resolve_concept(name)
            if canonical:
                name = canonical
        if name.lower() not in seen:
            seen.add(name.lower())
            results.append({
                "concept": name,
                "evidence": str(item.get("evidence", ""))[:500],
                "confidence": min(1.0, max(0.0, float(item.get("confidence", 0.7)))),
            })
    return results


def _extract_keywords_openai(llm, title: str, text: str,
                              sqlite_db=None) -> list[str]:
    """LLMìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œ 5~10ê°œ ì¶”ì¶œ + DB alias ì •ê·œí™” ì ìš©"""
    import json as _json

    prompt = (
        "You extract 5-10 core academic concepts/keywords from AI/ML papers. "
        "Return ONLY a JSON array of English concept names. "
        "Use standard academic terms (e.g. 'Transformer', 'Attention Mechanism', "
        "'Reinforcement Learning', 'Knowledge Distillation'). "
        "Always use SINGULAR form (e.g. 'Neural Network' not 'Neural Networks'). "
        "Use full names, not abbreviations (e.g. 'Large Language Model' not 'LLM'). "
        "Do NOT include LaTeX commands, paper-specific names, or generic terms like 'AI' or 'deep learning' unless central.\n\n"
        f"Paper: {title}\n\n{text[:2500]}"
    )
    raw = llm.generate(prompt).strip()
    if raw.startswith("```"):
        raw = re.sub(r"^```\w*\n?", "", raw)
        raw = re.sub(r"\n?```$", "", raw)
    keywords = _json.loads(raw)
    if not isinstance(keywords, list):
        return []

    result = []
    seen = set()
    for k in keywords:
        name = str(k).strip()
        if not name or len(name) <= 1:
            continue
        if sqlite_db:
            canonical = sqlite_db.resolve_concept(name)
            if canonical:
                name = canonical
        if name.lower() not in seen:
            seen.add(name.lower())
            result.append(name)
    return result


def _update_note_concepts(content: str, concepts: list[str]) -> str:
    """ë…¸íŠ¸ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì„¹ì…˜ì„ ì—…ë°ì´íŠ¸ ë˜ëŠ” ì¶”ê°€"""
    concept_lines = "# ğŸ§© ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë…\n- [[00_Concept_Index]]\n"
    for c in concepts:
        concept_lines += f"- [[{c}]]\n"

    placeholder = "ìš”ì•½ë³¸/ë²ˆì—­ë³¸ì´ ì•„ì§ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
    if placeholder in content:
        old_line_pattern = re.compile(r'.*ìš”ì•½ë³¸/ë²ˆì—­ë³¸ì´ ì•„ì§.*paper sync-keywords.*\n?', re.DOTALL)
        cleaned = content
        for line in content.split('\n'):
            if placeholder in line:
                cleaned = cleaned.replace(line, '')
                break
        for line in cleaned.split('\n'):
            if 'sync-keywords' in line:
                cleaned = cleaned.replace(line, '')
                break
        content = cleaned.rstrip() + "\n\n"

    if "ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë…" in content:
        pattern = re.compile(
            r'(#[#\s]*ğŸ§©?\s*ë‚´ê°€ ë°°ì›Œì•¼ í•  ê°œë….*?\n)((?:- \[\[.*?\]\]\n)*)',
            re.MULTILINE,
        )
        if pattern.search(content):
            content = pattern.sub(concept_lines, content)
        else:
            content = content.rstrip() + "\n\n" + concept_lines
    elif "í•µì‹¬ í‚¤ì›Œë“œ:" in content:
        kw_line = re.search(r'í•µì‹¬ í‚¤ì›Œë“œ:.*\n', content)
        if kw_line:
            content = content[:kw_line.start()] + concept_lines + content[kw_line.end():]
    else:
        content = content.rstrip() + "\n\n" + concept_lines

    return content


def _regenerate_concept_index(index_path: Path, all_concepts: dict[str, list[str]]):
    """00_Concept_Index.mdë¥¼ ë¹ˆë„ìˆœìœ¼ë¡œ ì¬ìƒì„±"""
    sorted_concepts = sorted(all_concepts.items(), key=lambda x: -len(x[1]))

    lines = [
        "---",
        "title: 00_Concept_Index",
        "---",
        "",
        "# AI Papers Concept Index",
        "",
        "ì´ í´ë” ë‚´ ìš”ì•½ ë…¸íŠ¸ì—ì„œ ì¶”ì¶œëœ ê°œë… ë§í¬ ëª©ë¡",
        "",
        "## ê°œë…",
    ]
    for concept, papers in sorted_concepts:
        lines.append(f"- [[{concept}]] ({len(papers)})")

    lines.append("")
    index_path.write_text("\n".join(lines), encoding="utf-8")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper build-concepts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("build-concepts")
@click.option("--force", is_flag=True, help="ê¸°ì¡´ ê°œë… ë…¸íŠ¸ë„ ì¬ìƒì„±")
@click.pass_context
def paper_build_concepts(ctx, force):
    """ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ê°œë³„ ê°œë… ë…¸íŠ¸ ìƒì„± + kg_relationsì— ê´€ê³„ ì €ì¥"""
    config = ctx.obj["khub"].config
    import json

    vault_path = config.vault_path
    if not vault_path:
        console.print("[red]Obsidian vault ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
        return

    papers_dir = _resolve_vault_papers_dir(vault_path)
    concepts_dir = _resolve_vault_concepts_dir(vault_path)
    concepts_dir.mkdir(parents=True, exist_ok=True)

    from knowledge_hub.providers.registry import get_llm as _get_llm
    prov = config.summarization_provider
    mdl = config.summarization_model
    prov_cfg = config.get_provider_config(prov)
    llm = _get_llm(prov, model=mdl, **prov_cfg)

    from knowledge_hub.core.database import SQLiteDatabase
    sqlite_db = SQLiteDatabase(config.sqlite_path)

    # 1) ëª¨ë“  ë…¼ë¬¸ ë…¸íŠ¸ì—ì„œ ê°œë… â†’ ë…¼ë¬¸ ë§¤í•‘ ìˆ˜ì§‘
    concept_papers: dict[str, list[str]] = {}
    md_files = sorted(papers_dir.glob("*.md"))
    for md_path in md_files:
        if md_path.name == "00_Concept_Index.md":
            continue
        content = md_path.read_text(encoding="utf-8")
        concepts = re.findall(r'\[\[([^\]]+)\]\]', content)
        for c in concepts:
            if c != "00_Concept_Index":
                concept_papers.setdefault(c, []).append(md_path.stem)

    all_concept_names = sorted(concept_papers.keys())
    console.print(f"[bold]{len(all_concept_names)}ê°œ ê°œë… ë°œê²¬[/bold]")

    if not force:
        existing = {f.stem for f in concepts_dir.glob("*.md")}
        to_process = [c for c in all_concept_names if c not in existing]
    else:
        to_process = list(all_concept_names)

    if not to_process:
        console.print("[green]ëª¨ë“  ê°œë… ë…¸íŠ¸ê°€ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. --forceë¡œ ì¬ìƒì„± ê°€ëŠ¥.[/green]")
        _rebuild_concept_index_with_relations(papers_dir, concepts_dir, concept_papers)
        return

    console.print(f"[bold]{len(to_process)}ê°œ ê°œë… ë…¸íŠ¸ ìƒì„± ì‹œì‘[/bold]\n")

    batch_size = 15
    created = 0
    relations_stored = 0

    for i in range(0, len(to_process), batch_size):
        batch = to_process[i:i + batch_size]
        console.print(f"  ë°°ì¹˜ [{i+1}~{i+len(batch)}/{len(to_process)}]...", end=" ")

        try:
            results = _batch_describe_concepts(llm, batch, all_concept_names)
        except Exception as e:
            console.print(f"[red]API ì˜¤ë¥˜: {e}[/red]")
            continue

        for concept_name, info in results.items():
            desc = info.get("description", "")
            related = info.get("related", [])
            papers = concept_papers.get(concept_name, [])

            cid = _concept_id(concept_name)
            sqlite_db.upsert_concept(cid, concept_name, desc)

            for rel_name in related:
                rel_id = _concept_id(rel_name)
                sqlite_db.upsert_concept(rel_id, rel_name)
                sqlite_db.add_relation(
                    source_type="concept", source_id=cid,
                    relation="concept_related_to",
                    target_type="concept", target_id=rel_id,
                    evidence_text=f"LLMì´ {concept_name}ì˜ ê´€ë ¨ ê°œë…ìœ¼ë¡œ ì‹ë³„",
                    confidence=0.6,
                )
                relations_stored += 1

            note_content = _build_concept_note(concept_name, desc, related, papers)
            safe_name = re.sub(r'[\\/:*?"<>|]', '', concept_name).strip()
            note_path = concepts_dir / f"{safe_name}.md"
            note_path.write_text(note_content, encoding="utf-8")
            created += 1

        console.print(f"[green]{len(results)}ê°œ ìƒì„±[/green]")

    _rebuild_concept_index_with_relations(papers_dir, concepts_dir, concept_papers)

    console.print(f"\n[bold green]{created}ê°œ ê°œë… ë…¸íŠ¸ ìƒì„± ì™„ë£Œ[/bold green]")
    console.print(f"[dim]concept_related_to ê´€ê³„: {relations_stored}ê°œ ì €ì¥[/dim]")
    console.print(f"[dim]ìœ„ì¹˜: {concepts_dir}[/dim]")


def _batch_describe_concepts(llm, batch: list[str], all_concepts: list[str]) -> dict:
    """LLMìœ¼ë¡œ ê°œë… ë°°ì¹˜ì˜ ì„¤ëª… + ê´€ë ¨ ê°œë… ì¶”ì¶œ"""
    import json

    concept_list_str = ", ".join(all_concepts[:200])

    prompt = (
        "You are an AI/ML concept expert. For each concept, provide:\n"
        "1. A concise Korean description (1-2 sentences) explaining what it is\n"
        "2. 3-5 related concepts from the provided concept list\n\n"
        "Return ONLY valid JSON: {\"ConceptName\": {\"description\": \"í•œêµ­ì–´ ì„¤ëª…\", \"related\": [\"Related1\", \"Related2\", ...]}, ...}\n"
        "Pick related concepts ONLY from the provided list. Be precise and educational.\n\n"
        f"Concepts to describe:\n{json.dumps(batch, ensure_ascii=False)}\n\n"
        f"Available concepts for relations:\n{concept_list_str}"
    )
    raw = llm.generate(prompt).strip()
    if raw.startswith("```"):
        raw = re.sub(r"^```\w*\n?", "", raw)
        raw = re.sub(r"\n?```$", "", raw)
    return json.loads(raw)


def _build_concept_note(name: str, description: str, related: list[str], papers: list[str]) -> str:
    """ê°œë³„ ê°œë… ë…¸íŠ¸ ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
    lines = [
        "---",
        "type: concept",
        f'title: "{name}"',
        "---",
        "",
        f"# {name}",
        "",
        description,
        "",
    ]

    if related:
        lines.append("## ê´€ë ¨ ê°œë…")
        for r in related:
            lines.append(f"- [[{r}]]")
        lines.append("")

    if papers:
        lines.append("## ê´€ë ¨ ë…¼ë¬¸")
        for p in papers:
            lines.append(f"- [[{p}]]")
        lines.append("")

    lines.append(f"*[[00_Concept_Index|â† ê°œë… ëª©ë¡ìœ¼ë¡œ]]*")
    lines.append("")
    return "\n".join(lines)


def _rebuild_concept_index_with_relations(papers_dir: Path, concepts_dir: Path, concept_papers: dict[str, list[str]]):
    """Concept Indexë¥¼ ê´€ê³„ ì •ë³´ í¬í•¨í•˜ì—¬ ì¬ìƒì„±"""
    sorted_concepts = sorted(concept_papers.items(), key=lambda x: -len(x[1]))

    has_note = {f.stem for f in concepts_dir.glob("*.md")}

    lines = [
        "---",
        "title: 00_Concept_Index",
        "---",
        "",
        "# AI Papers Concept Index",
        "",
        "ì´ í´ë” ë‚´ ìš”ì•½ ë…¸íŠ¸ì—ì„œ ì¶”ì¶œëœ ê°œë… ë§í¬ ëª©ë¡",
        f"ì´ **{len(sorted_concepts)}ê°œ** ê°œë… | **{len(has_note)}ê°œ** ì„¤ëª… ë…¸íŠ¸ ìƒì„±ë¨",
        "",
    ]

    freq_groups = {"## í•µì‹¬ ê°œë… (3íšŒ ì´ìƒ)": [], "## ì£¼ìš” ê°œë… (2íšŒ)": [], "## ê¸°íƒ€ ê°œë… (1íšŒ)": []}
    for concept, papers in sorted_concepts:
        count = len(papers)
        status = "ğŸ“" if concept in has_note else "ğŸ“Œ"
        entry = f"- {status} [[{concept}]] ({count}í¸)"
        if count >= 3:
            freq_groups["## í•µì‹¬ ê°œë… (3íšŒ ì´ìƒ)"].append(entry)
        elif count == 2:
            freq_groups["## ì£¼ìš” ê°œë… (2íšŒ)"].append(entry)
        else:
            freq_groups["## ê¸°íƒ€ ê°œë… (1íšŒ)"].append(entry)

    for heading, entries in freq_groups.items():
        if entries:
            lines.append(heading)
            lines.extend(entries)
            lines.append("")

    lines.append("")
    (papers_dir / "00_Concept_Index.md").write_text("\n".join(lines), encoding="utf-8")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper normalize-concepts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("normalize-concepts")
@click.option("--dry-run", is_flag=True, help="ë³€ê²½ ì—†ì´ íƒì§€ ê²°ê³¼ë§Œ í‘œì‹œ")
@click.pass_context
def paper_normalize_concepts(ctx, dry_run):
    """ê°œë… ë™ì˜ì–´/ë³µìˆ˜í˜•/ì•½ì–´ íƒì§€ â†’ ì •ê·œí™” + ë³‘í•©"""
    config = ctx.obj["khub"].config
    import json as _json
    from knowledge_hub.core.database import SQLiteDatabase

    vault_path = config.vault_path
    if not vault_path:
        console.print("[red]Obsidian vault ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
        return

    from knowledge_hub.providers.registry import get_llm as _get_llm
    prov = config.summarization_provider
    mdl = config.summarization_model
    prov_cfg = config.get_provider_config(prov)
    llm = _get_llm(prov, model=mdl, **prov_cfg)

    papers_dir = _resolve_vault_papers_dir(vault_path)
    concepts_dir = _resolve_vault_concepts_dir(vault_path)

    # 1) ëª¨ë“  ê°œë… ì´ë¦„ ìˆ˜ì§‘
    concept_names = sorted({f.stem for f in concepts_dir.glob("*.md")}) if concepts_dir.exists() else []

    md_files = sorted(papers_dir.glob("*.md"))
    for md_path in md_files:
        if md_path.name == "00_Concept_Index.md":
            continue
        content = md_path.read_text(encoding="utf-8")
        for c in re.findall(r'\[\[([^\]]+)\]\]', content):
            if c != "00_Concept_Index" and c not in concept_names:
                concept_names.append(c)

    concept_names = sorted(set(concept_names))
    console.print(f"[bold]{len(concept_names)}ê°œ ê°œë… ìŠ¤ìº” ì™„ë£Œ[/bold]\n")

    if len(concept_names) < 2:
        console.print("[green]ì •ê·œí™”í•  ê°œë…ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.[/green]")
        return

    # 2) LLMìœ¼ë¡œ ë™ì˜ì–´ ê·¸ë£¹ íƒì§€ (ë°°ì¹˜)
    console.print("[bold]ë™ì˜ì–´/ë³µìˆ˜í˜•/ì•½ì–´ ê·¸ë£¹ íƒì§€ ì¤‘...[/bold]")
    all_groups: list[dict] = []
    batch_size = 80

    for i in range(0, len(concept_names), batch_size):
        batch = concept_names[i:i + batch_size]
        console.print(f"  ë°°ì¹˜ [{i+1}~{i+len(batch)}/{len(concept_names)}]...", end=" ")
        try:
            groups = _detect_synonym_groups(llm, batch)
            all_groups.extend(groups)
            console.print(f"[green]{len(groups)}ê°œ ê·¸ë£¹[/green]")
        except Exception as e:
            console.print(f"[red]ì‹¤íŒ¨: {e}[/red]")

    if not all_groups:
        console.print("[green]ë™ì˜ì–´ ê·¸ë£¹ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/green]")
        return

    # 3) ê²°ê³¼ í‘œì‹œ
    table = Table(title=f"ë™ì˜ì–´ ê·¸ë£¹ ({len(all_groups)}ê°œ)")
    table.add_column("ì •ê·œ ì´ë¦„", style="cyan")
    table.add_column("ë³„ì¹­ (ë³‘í•© ëŒ€ìƒ)", style="yellow")
    for g in all_groups:
        table.add_row(g["canonical"], ", ".join(g["aliases"]))
    console.print(table)

    if dry_run:
        console.print("\n[dim]--dry-run: ë³€ê²½ ì—†ì´ ì¢…ë£Œ[/dim]")
        return

    # 4) SQLiteì— concepts + aliases ë“±ë¡
    sqlite_db = SQLiteDatabase(config.sqlite_path)
    registered = 0

    for name in concept_names:
        cid = _concept_id(name)
        sqlite_db.upsert_concept(cid, name)

    for g in all_groups:
        canonical = g["canonical"]
        canonical_id = _concept_id(canonical)
        sqlite_db.upsert_concept(canonical_id, canonical)

        for alias in g["aliases"]:
            sqlite_db.add_alias(alias, canonical_id)

            alias_id = _concept_id(alias)
            existing = sqlite_db.get_concept(alias_id)
            if existing and existing["canonical_name"] != canonical:
                sqlite_db.delete_concept(alias_id)

        registered += 1

    console.print(f"\n[green]{registered}ê°œ ì •ê·œí™” ê·¸ë£¹ DB ë“±ë¡[/green]")

    # 5) Obsidian ë…¸íŠ¸ ë³‘í•© + ë…¼ë¬¸ ë…¸íŠ¸ ì¹˜í™˜
    merged = 0
    for g in all_groups:
        canonical = g["canonical"]
        for alias in g["aliases"]:
            merged += _merge_obsidian_concept(papers_dir, concepts_dir, alias, canonical)
            _replace_in_paper_notes(papers_dir, alias, canonical)

    console.print(f"[green]Obsidian ë…¸íŠ¸ {merged}ê°œ ë³‘í•© ì™„ë£Œ[/green]")

    # 6) Concept Index ì¬ìƒì„±
    concept_papers: dict[str, list[str]] = {}
    for md_path in sorted(papers_dir.glob("*.md")):
        if md_path.name == "00_Concept_Index.md":
            continue
        content = md_path.read_text(encoding="utf-8")
        for c in re.findall(r'\[\[([^\]]+)\]\]', content):
            if c != "00_Concept_Index":
                concept_papers.setdefault(c, []).append(md_path.stem)

    _rebuild_concept_index_with_relations(papers_dir, concepts_dir, concept_papers)
    console.print(f"[bold green]ì •ê·œí™” ì™„ë£Œ â€” {len(all_groups)}ê°œ ê·¸ë£¹, {merged}ê°œ ë…¸íŠ¸ ë³‘í•©[/bold green]")


def _concept_id(name: str) -> str:
    """ê°œë… ì´ë¦„ì—ì„œ ì•ˆì •ì ì¸ ID ìƒì„± (ì†Œë¬¸ì, ê³µë°±â†’ì–¸ë”ìŠ¤ì½”ì–´)"""
    return re.sub(r'\s+', '_', name.strip()).lower()


def _detect_synonym_groups(llm, concept_names: list[str]) -> list[dict]:
    """LLMìœ¼ë¡œ ë™ì˜ì–´/ë³µìˆ˜í˜•/ì•½ì–´ ê·¸ë£¹ íƒì§€"""
    import json as _json

    prompt = (
        "You are an AI/ML terminology expert. Given a list of concept names, "
        "find groups of synonyms, abbreviations, plural/singular variants, or "
        "near-duplicates that should be merged into a single canonical concept.\n\n"
        "Rules:\n"
        "- Only group terms that truly refer to the SAME concept\n"
        "- Do NOT merge parent-child (e.g. 'Reinforcement Learning' and 'Multi-Agent RL' are different)\n"
        "- Prefer singular form as canonical\n"
        "- Prefer full name over abbreviation as canonical\n"
        "- Return ONLY a JSON array of {\"canonical\": \"...\", \"aliases\": [\"...\"]}\n"
        "- Skip concepts with no duplicates\n\n"
        + _json.dumps(concept_names, ensure_ascii=False)
    )
    raw = llm.generate(prompt).strip()
    if raw.startswith("```"):
        raw = re.sub(r"^```\w*\n?", "", raw)
        raw = re.sub(r"\n?```$", "", raw)
    groups = _json.loads(raw)
    if not isinstance(groups, list):
        return []
    return [g for g in groups if isinstance(g, dict) and g.get("canonical") and g.get("aliases")]


def _merge_obsidian_concept(papers_dir: Path, concepts_dir: Path, alias: str, canonical: str) -> int:
    """Obsidian ê°œë… ë…¸íŠ¸ ë³‘í•©: alias ë…¸íŠ¸ì˜ ê´€ë ¨ ë…¼ë¬¸/ê°œë…ì„ canonicalì— í•©ì‚° í›„ ì‚­ì œ"""
    safe_alias = re.sub(r'[\\/:*?"<>|]', '', alias).strip()
    safe_canonical = re.sub(r'[\\/:*?"<>|]', '', canonical).strip()
    alias_path = concepts_dir / f"{safe_alias}.md"
    canonical_path = concepts_dir / f"{safe_canonical}.md"

    if not alias_path.exists():
        return 0

    alias_content = alias_path.read_text(encoding="utf-8")
    alias_papers = set(re.findall(r'\[\[([^\]]+)\]\]', alias_content))

    if canonical_path.exists():
        can_content = canonical_path.read_text(encoding="utf-8")
        can_papers = set(re.findall(r'\[\[([^\]]+)\]\]', can_content))
        new_papers = alias_papers - can_papers - {canonical, "00_Concept_Index"}

        if new_papers and "## ê´€ë ¨ ë…¼ë¬¸" in can_content:
            insert_point = can_content.index("## ê´€ë ¨ ë…¼ë¬¸") + len("## ê´€ë ¨ ë…¼ë¬¸")
            next_nl = can_content.index("\n", insert_point)
            extra = "\n".join(f"- [[{p}]]" for p in sorted(new_papers))
            can_content = can_content[:next_nl] + "\n" + extra + can_content[next_nl:]
            canonical_path.write_text(can_content, encoding="utf-8")

    alias_path.unlink()
    return 1


def _replace_in_paper_notes(papers_dir: Path, old_name: str, new_name: str):
    """ëª¨ë“  ë…¼ë¬¸ ë…¸íŠ¸ì—ì„œ [[old_name]] â†’ [[new_name]] ì¹˜í™˜"""
    old_link = f"[[{old_name}]]"
    new_link = f"[[{new_name}]]"
    for md_path in papers_dir.glob("*.md"):
        if md_path.name == "00_Concept_Index.md":
            continue
        content = md_path.read_text(encoding="utf-8")
        if old_link in content:
            content = content.replace(old_link, new_link)
            md_path.write_text(content, encoding="utf-8")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# paper info <arxiv_id>
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@paper_group.command("info")
@click.argument("arxiv_id")
@click.pass_context
def paper_info(ctx, arxiv_id):
    """ë…¼ë¬¸ ìƒì„¸ ì •ë³´"""
    arxiv_id = _validate_arxiv_id(arxiv_id)
    config = ctx.obj["khub"].config
    from knowledge_hub.core.database import SQLiteDatabase

    sqlite_db = SQLiteDatabase(config.sqlite_path)
    paper = sqlite_db.get_paper(arxiv_id)

    if not paper:
        console.print(f"[red]ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arxiv_id}[/red]")
        return

    table = Table(title=f"ë…¼ë¬¸ ì •ë³´: {arxiv_id}")
    table.add_column("í•­ëª©", style="cyan", width=12)
    table.add_column("ê°’")

    table.add_row("ì œëª©", paper["title"])
    table.add_row("ì €ì", paper.get("authors", ""))
    table.add_row("ì—°ë„", str(paper.get("year", "")))
    table.add_row("ë¶„ì•¼", paper.get("field", ""))
    table.add_row("ì¤‘ìš”ë„", str(paper.get("importance", "")))
    table.add_row("PDF", paper.get("pdf_path") or "-")
    table.add_row("í…ìŠ¤íŠ¸", paper.get("text_path") or "-")
    table.add_row("ë²ˆì—­", paper.get("translated_path") or "-")
    table.add_row("ì¸ë±ì‹±", "O" if paper.get("indexed") else "-")
    table.add_row("arXiv", f"https://arxiv.org/abs/{arxiv_id}")

    console.print(table)

    notes = paper.get("notes", "")
    if notes and len(notes) > 30:
        console.print(f"\n[bold]ìš”ì•½:[/bold]")
        console.print(notes[:500])
